---
title: "Predict Climate Impact on Pollinators"
author: "Jianing Ding"
date: "2024-06-30"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(raster)
library(ggplot2)
library(dplyr)
library(data.table)
library(rworldmap) 
library(rworldxtra)
library(lme4)
library(cowplot)
library(viridis)
library(ncdf4)
library(nlme)
library(sf)

# read in the original predicts database 
PREDICTS <- readRDS("C:\\Users\\dingj\\Desktop\\project\\predict data_use\\PREDICTS_pollinators_8_exp.rds") %>%
  mutate(COL_ID = as.character(COL_ID))

```

\#**Define Predict Continuous Function**

```{r function Continuous, echo=TRUE}
# Function to predict continuous values based on the model
predict_continuous <- function(model,
                               model_data, 
                               response_variable,
                               categorical_variable,
                               continuous_variable,
                               continuous_transformation,
                               random_variable,
                               colour_palette) {
  
  # Set up the prediction dataframe with necessary variables
  prediction_data <- model_data[, c(response_variable, random_variable, categorical_variable, continuous_variable)]
  
  # Remove any incomplete rows (NAs) from the prediction data
  prediction_data <- prediction_data[complete.cases(prediction_data),]
  
  # Ensure all factor levels are the same as in the model for both categorical and random variables
  factor_vars <- c(categorical_variable, random_variable)
  for (var in factor_vars) {
    if (is.factor(prediction_data[[var]])) {
      levels_model <- if (inherits(model, "lme")) {
        levels(model_data[[var]])
      } else {
        levels(model@frame[[var]])
      }
      if (!all(levels(prediction_data[[var]]) %in% levels_model)) {
        prediction_data[[var]] <- factor(prediction_data[[var]], levels = levels_model)
      }
    }
  }
  
  # Predict the values for the model using synchronized data
  y_values <- tryCatch({
    if (inherits(model, "lme")) {
      # For lme model
      predict(model, newdata = prediction_data, level = 0)
    } else {
      # For lmer model
      StatisticalModels::PredictGLMER(model, data = prediction_data, se.fit = TRUE, seMultiplier = 1.96)
    }
  }, error = function(e) {
    print(paste("Error during prediction for variable", var, ":", e$message))
    NULL
  })
  
  # Check if prediction was successful
  if (is.null(y_values)) {
    print("Prediction failed, skipping to next iteration")
    return(NULL)  # Return NULL to indicate failure
  }
  
  # Extract predicted values and calculate standard errors if necessary
  if (inherits(model, "lme")) {
    y_value <- y_values
    # Manually compute standard errors
    X <- model.matrix(formula(model), prediction_data)
    beta <- fixef(model)
    se.fit <- sqrt(rowSums((X %*% vcov(model)) * X))
    y_value_plus <- y_value + 1.96 * se.fit
    y_value_minus <- y_value - 1.96 * se.fit
  } else {
    y_value <- y_values[[1]]
    y_value_plus <- y_values[[2]]
    y_value_minus <- y_values[[3]]
  }
  
  # Bind the predicted values to the prediction data
  bound_values <- data.frame(cbind(prediction_data,
                                   y_value, 
                                   y_value_plus, 
                                   y_value_minus, 
                                   metric = response_variable,
                                   prediction_data[, continuous_variable]))
  
  # Rename columns
  colnames(bound_values)[ncol(bound_values)] <- paste(continuous_variable, "transform", sep = "_")
  bound_values <- dplyr::rename(bound_values, "response_variable" = all_of(response_variable))
  
  # Print the final dataframe of predicted values
  print(bound_values)
  
  return(bound_values)  # Ensure the function returns the result
}


```

\#**Define Calculate Baseline Function**

```{r Baseline Function, echo=TRUE}
# Function to calculate baseline (mean and sd)
calc_baseline <- function(data_file, func, pred_points, pred_points_sp){
  
  # Calculate either the mean or standard error for baseline, then extract points for predicts sites
  data_fin <- calc(data_file, func) %>%
    extract(pred_points_sp)
  
  # Bind the extracted values back onto the predicts coordinates
  data_fin <- data.frame(pred_points[,1:3 ], data_fin)
  
  return(data_fin)
}

```

\#**Load Climate Data**

```{r Load climate data, echo=TRUE}
# Load in the mean temperature data from CRU
tmp <- raster::stack("C:\\Users\\dingj\\Desktop\\project\\climate data\\cru_ts4.03.1901.2018.tmp.dat.nc", varname="tmp")

```

\#**Filter Pollinators Data**

```{r filter data, echo=TRUE}
# Read in the predicts pollinators
PREDICTS_pollinators_orig <- readRDS("C:\\Users\\dingj\\Desktop\\project\\predict data_use\\PREDICTS_pollinators_8_exp.rds") %>%
  dplyr::select(-clade_rank, -confidence)  %>%
  filter(Class == "Insecta") %>%
  dplyr::filter(Predominant_land_use %in% c("Cropland", "Primary vegetation")) %>%
  droplevels()

```

\#**Create Species Table**

```{r create table, echo=TRUE}
# Create table of number of unique species
PREDICTS_pollinators_orig %>%
  select(Class, Order, Best_guess_binomial) %>%
  filter(Best_guess_binomial != "") %>%
  unique() %>%
  group_by(Class, Order) %>%
  tally()

# Number of unique sites per land use
PREDICTS_pollinators_orig %>%
  select(Predominant_land_use, SSBS) %>%
  unique() %>%
  group_by(Predominant_land_use) %>%
  tally()

```

\#**Calculate Sampling Variation**

```{r calculate variation, echo=TRUE}
# Calculate variation in sampling effect
PREDICTS_pollinators_orig %>%
  group_by(SS) %>%
  summarise(sampling_variation = sd(Sampling_effort, na.rm = TRUE)) %>%
  mutate(varies = ifelse(sampling_variation > 0, 1, 0)) %>%
  mutate(rows = 1) %>%
  summarise(total_vary = sum(varies, na.rm = TRUE), total = sum(rows)) %>%
  mutate(percentage = (total_vary / total) * 100)

```

\#**Setup List**

```{r setup list, echo=TRUE}
# Set up vector for filtering for vertebrates and invertebrates
predict_climate_list <- list()
PREDICTS_pollinators_orig <- list(PREDICTS_pollinators_orig)

```

\#**Loop Through Phylum**

```{r loopOne, echo=TRUE}

  # PREDICTS data compilation
  # filter for main pollinating taxa
  PREDICTS_pollinators <- PREDICTS_pollinators_orig[[1]]
  
  # correct for sampling effort 标准化采样努力，以便不同站点或记录之间可以公平比较
  PREDICTS_pollinators <- predictsFunctions::CorrectSamplingEffort(PREDICTS_pollinators)
  
  # calculate site metrics including all species (confirmed and not confirmed pollinator)
  order.sites.div <-predictsFunctions::SiteMetrics(diversity = PREDICTS_pollinators,
                                                   extra.cols = c("SSB", "SSBS", "Predominant_land_use", "UN_region", "Pollinating"))
  
  
  # set id column for merging back into correct place设置合并用的ID列
  order.sites.div$id_col <- 1:nrow(order.sites.div)  
  
  # PREDICTS sites with the month of the recording提取和转换日期
  PRED_sites <- order.sites.div %>% select(id_col, Latitude, Longitude, Sample_end_latest) %>%
    mutate(Sample_end_latest = paste("X", substr(Sample_end_latest, start = 1, stop = 7), sep = "")) %>%
    mutate(Sample_end_latest = gsub("-", ".", Sample_end_latest)) %>%
    filter(!is.na(Latitude))  #去除那些纬度数据为缺失的记录
  
  # calculate the means and standard deviation for the beginning of the series
  # take names of values for 1901 to 1930
  tmp1901_1930 <- tmp[[names(tmp)[1:360]]]  
  
  # extract the points for each the predicts coordinates提取地理坐标并创建空间点
  PRED_sites_sp <- PRED_sites %>%
    select(Longitude, Latitude) %>%
    filter(!is.na(Latitude)) %>%
    SpatialPoints() #用于根据经纬度数据创建空间点对象
  
  # calculate the mean baseline, and convert to character for merging 计算平均基线值并转换为字符类型
  climate_start_mean <- calc_baseline(tmp1901_1930, 
                                      func = mean, 
                                      pred_points = PRED_sites, #站点数据
                                      pred_points_sp = PRED_sites_sp) %>% #空间点数据
    mutate(Latitude = as.character(Latitude)) %>%
    mutate(Longitude = as.character(Longitude))
  
  # calculate the sd baseline, and convert to character for merging 计算标准偏差基线值并转换为字符类型
  climate_start_sd <- calc_baseline(tmp1901_1930, 
                                    func = stats::sd, 
                                    pred_points = PRED_sites, 
                                    pred_points_sp = PRED_sites_sp) %>%
    mutate(Latitude = as.character(Latitude)) %>%
    mutate(Longitude = as.character(Longitude))
  
  #以上代码：处理和分析基于地理位置的气候数据，计算出特定时间段内的平均气候基线和气候变异性
  
  # calculate the mean temperatures for each predicts site, 11 months previously
  # set up empty list for each dataframe
  raster_means <- list()  
  
  # time the length of the loop over each unique date in predicts sites
  system.time(
    # for each unique site month, select the pixels for that date, and 11 months previously, and then convert each raster set to a dataframe
    # 计算每个预测站点在过去11个月内的平均温度
    
    for(i in 1:length(unique(PRED_sites$Sample_end_latest))){
      
      # create unique list of the end dates for the predicts sites
      pred_dates <- unique(PRED_sites$Sample_end_latest)
      
      # select the raster pixels for the end date of the predicts site, and then the index of that name
      date_raster <- grepl(pred_dates[i], names(tmp))#检查 tmp 数据集（可能是一个包含多个栅格层的列表或数组）的名称中是否包含 pred_dates[i]（当前循环的日期）。grepl 返回一个逻辑向量，表明哪些元素匹配。
      site_index <- which(date_raster == TRUE)#找出上述逻辑向量中为 TRUE 的索引，这些索引指向与当前日期匹配的栅格层。
      
      # select the previous 11 indices - i.e. 11 months previous worth of pixels
      ind_raster <- tmp[[names(tmp)[(site_index - 11): site_index]]] #使用这些索引来从 tmp 中选择对应的栅格数据，这包括了从当前月份往前数的11个月的数据。names(tmp)[(site_index - 11): site_index] 计算得到需要的栅格层的名称，然后从 tmp 中提取这些层。
      
      # filter the raster for only coordinates we have PREDICTS sites for that date
      # 筛选特定日期的预测站点并创建空间点
      PRED_sites_filt <- PRED_sites %>%
        filter(Sample_end_latest == pred_dates[i]) %>%
        select(Longitude, Latitude) %>%
        SpatialPoints() #将筛选后的经度和纬度转换为空间点对象，这对于后续的空间数据分析非常重要。
      
      # identify site ids for merging 识别站点ID以便合并
      site_ids <- PRED_sites %>%
        filter(Sample_end_latest == pred_dates[i]) %>%
        select(id_col, Longitude, Latitude)
      
      # filter the raster for that date for the locations we have predicts sites根据预测站点位置筛选栅格数据
      PRED_coords <- cbind(site_ids, extract(ind_raster, PRED_sites_filt, na.rm = FALSE))
      
      # convert that set of dates to a dataframe
      ind_raster_frame <- as.data.frame(PRED_coords)
      
      # remove the extra coordinate columns for calculating the row means 移除额外的坐标列
      ind_raster_values <- ind_raster_frame %>% select(-id_col, -Longitude, -Latitude)
      
      # calculate the mean values for each coordinate and bind back onto the coordinates 计算每个坐标的平均值并重新绑定
      raster_means[[i]] <- cbind(names(tmp)[site_index], (ind_raster_frame %>% select(id_col, Longitude, Latitude)), rowMeans(ind_raster_values))
      colnames(raster_means[[i]]) <- c("end_date", "id_col", "x", "y", "mean_value")
      
      # print the iteration number
      print(i)
    }
    #目的是为每个独特的样本结束日期计算相关预测站点的平均温度，并将这些信息与站点的地理坐标一起存储
  )  
  
  ## adjust the mean value for each site for the baseline at that site
  #如何调整每个站点的平均温度值，使其与该站点的基线（即早期记录的平均值和标准偏差）相对应。
  #这是一个重要的分析步骤，用于计算温度异常和标准化温度异常
  #可以帮助了解气候变化的具体影响。
  # first, merge the baseline sd and mean by coordinate for each site
  adjusted_climate <- rbindlist(raster_means) %>%
    select(-end_date) %>%
    unique() %>%
    inner_join(climate_start_mean, by = "id_col") %>%#为每个站点添加了基线平均温度 
    rename("mean_base" = "data_fin") %>%
    inner_join(climate_start_sd, by = "id_col") %>% #再次进行内连接，这次是添加基线标准偏差 
    rename("sd_base" = "data_fin") %>%
    mutate(anomaly = mean_value - mean_base) %>% #表示温度相对于历史平均的偏差
    mutate(standard_anom = anomaly / sd_base) #标准化的异常值表示每个站点的温度偏差与历史变异性的比例，有助于在不同站点间进行比较
  
  # bind the adjusted climate data back onto the predicts sites 
  # 将气候异常值和标准化异常值与其他生态和地理数据结合，为后续的生态分析提供一个综合的数据集
  predicts_climate <- inner_join(order.sites.div, adjusted_climate, by = "id_col")
  
  # add 1 for abundance and simpson diversity
  predicts_climate$Total_abundance <- predicts_climate$Total_abundance + 1
  predicts_climate$Simpson_diversity <- predicts_climate$Simpson_diversity + 1
  
  predicts_climate_sf <- st_as_sf(predicts_climate, coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant")
  
  # Transform to UTM coordinates (assuming zone 60 is being used here)
  predicts_climate_sf <- st_transform(predicts_climate_sf, crs = st_crs("+proj=utm +zone=60 +datum=WGS84 +units=m +ellps=WGS84"))
  
  # Add the UTM coordinates back to the predicts_climate data frame
  predicts_climate$UTM_Easting <- st_coordinates(predicts_climate_sf)[, 1]
  predicts_climate$UTM_Northing <- st_coordinates(predicts_climate_sf)[, 2]
  
  
  
  # assign to list of predicts_climate and insects and vertebrates
  predict_climate_list[[1]] <- predicts_climate
  

#这个列通常代表在某个站点观察到的所有物种的总数
siteTotalAbund_variable <- order.sites.div$Total_abundance

# 检查 siteTotalAbund 变量的值
#print(siteTotalAbund_variable)

```

\#**Analyse on different spatial structures**

```{r Spatial analysis, echo=TRUE}
for(m in 1:length(predict_climate_list)) {
  data_for_model <- predict_climate_list[[m]]
  set.seed(123) 
  data_for_model$UTM_Easting <- jitter(data_for_model$UTM_Easting, factor = 1e-5)
  data_for_model$UTM_Northing <- jitter(data_for_model$UTM_Northing, factor = 1e-5)
  
  # 更新列表中的数据
  predict_climate_list[[m]] <- data_for_model
}


# 运行多个模型以比较不同的空间结构
for(m in 1:length(predict_climate_list)){
  data_for_model <- predict_climate_list[[m]]
  data_for_model <- na.omit(data_for_model)  # 移除含有缺失值的行
  
  if(sum(is.na(data_for_model)) > 0) {
    print(paste("Data set", m, "still contains NA values."))
  } else {
    # 高斯空间结构
    model_gaussian_random <- lme(
      log(Total_abundance) ~ standard_anom * Predominant_land_use,
      random = ~ 1 | SS/SSB,
      data = data_for_model,
      correlation = corGaus(form = ~ UTM_Easting + UTM_Northing)
    )
    print(paste("Gaussian structure model for dataset", m))
    print(summary(model_gaussian_random))
    
    # 指数空间结构
    model_exponential_random <- lme(
      log(Total_abundance) ~ standard_anom * Predominant_land_use,
      random = ~ 1 | SS/SSB,
      data = data_for_model,
      correlation = corExp(form = ~ UTM_Easting + UTM_Northing)
    )
    print(paste("Exponential structure model for dataset", m))
    print(summary(model_exponential_random))
    
    # 球形空间结构
    model_spherical_random <- lme(
      log(Total_abundance) ~ standard_anom * Predominant_land_use,
      random = ~ 1 | SS/SSB,
      data = data_for_model,
      correlation = corSpher(form = ~ UTM_Easting + UTM_Northing)
    )
    print(paste("Spherical structure model for dataset", m))
    print(summary(model_spherical_random))
  }
}

# Compare AIC of different models举例比较AIC
print(paste("AIC for Gaussian model:", AIC(model_gaussian_random)))
print(paste("AIC for Exponential model:", AIC(model_exponential_random)))
print(paste("AIC for Spherical model:", AIC(model_spherical_random)))

```


```{r Plot duplicate coordinates, echo=TRUE}

# Identify duplicate UTM coordinates
utm_duplicated_coords <- predicts_climate[duplicated(predicts_climate[, c("UTM_Easting", "UTM_Northing")]), ]
#print("Duplicated UTM Coordinates:")
#print(utm_duplicated_coords)

# Retrieve all entries with duplicated coordinates
all_duplicated_coords <- predicts_climate[predicts_climate$UTM_Easting %in% utm_duplicated_coords$UTM_Easting & predicts_climate$UTM_Northing %in% utm_duplicated_coords$UTM_Northing, ]
#print("All duplicated coordinates:")
#print(all_duplicated_coords)
install.packages("rnaturalearth")
install.packages("rnaturalearthdata")
library(rnaturalearth)
library(rnaturalearthdata)

# Convert the original data to an sf object
original_sf <- st_as_sf(predicts_climate, coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant")

# Convert duplicated data to an sf object
if(nrow(all_duplicated_coords) > 0) {
  duplicated_sf <- st_as_sf(all_duplicated_coords, coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant")
  
world <- ne_countries(scale = "medium", returnclass = "sf")

ggplot() +
  geom_sf(data = world, fill = "gray90", color = "gray60") +
  geom_sf(data = original_sf, aes(color = "Original"), size = 1, alpha = 0.5) +  # 使用aes添加图例项
  geom_sf(data = duplicated_sf, aes(color = "Duplicated"), size = 2, shape = 21, fill = "#D89090") +  # 使用aes添加图例项
  scale_color_manual(values = c("Original" = "#758CCE", "Duplicated" = "#D80900"), 
                     name = "Site Type", 
                     labels = c("Original", "Duplicated")) +  # 自定义颜色和图例名称
  theme_minimal() +
  labs(title = "Map of Original and Duplicated Sites",
       x = "Longitude",
       y = "Latitude")


} else {
  print("No duplicated coordinates found in the dataset.")
}
ggsave("Map_of_Original_and_Duplicated_Sites.png", plot = last_plot(), width = 10, height = 8, units = "in")


```


```{r create spatial models, echo=TRUE}
# Set a seed for reproducibility
set.seed(123)

predict_climate_list[[1]]$UTM_Easting <- jitter(predict_climate_list[[1]]$UTM_Easting, factor = 1e-5)
predict_climate_list[[1]]$UTM_Northing <- jitter(predict_climate_list[[1]]$UTM_Northing, factor = 1e-5)

data_spatial <- predict_climate_list[[1]]
data_spatial <- na.omit(data_spatial)  

# Check for any remaining NA values
if(sum(is.na(data_spatial)) > 0) {
    print("Data set still contains NA values.")
} else {
    # Gaussian spatial structure
    model_gaussian_random <- lme(
      log(Total_abundance) ~ standard_anom * Predominant_land_use,
      random = list(SS = ~ 1, SSB = ~ 1),
      data = data_spatial,
      correlation = corGaus(form = ~ UTM_Easting + UTM_Northing)
    )
    print("Gaussian structure model for dataset")
    print(summary(model_gaussian_random))
    
    # Exponential spatial structure
    model_exponential_random <- lme(
      log(Total_abundance) ~ standard_anom * Predominant_land_use,
      random = list(SS = ~ 1, SSB = ~ 1),
      data = data_spatial,
      correlation = corExp(form = ~ UTM_Easting + UTM_Northing)
    )
    print("Exponential structure model for dataset")
    print(summary(model_exponential_random))
    
    # Spherical spatial structure
    model_spherical_random <- lme(
      log(Total_abundance) ~ standard_anom * Predominant_land_use,
      random = list(SS = ~ 1, SSB = ~ 1),
      data = data_spatial,
      correlation = corSpher(form = ~ UTM_Easting + UTM_Northing)
    )
    print("Spherical structure model for dataset")
    print(summary(model_spherical_random))
}

print(paste("AIC for Gaussian model:", AIC(model_gaussian_random)))
print(paste("AIC for Exponential model:", AIC(model_exponential_random)))
print(paste("AIC for Spherical model:", AIC(model_spherical_random)))


```

```{r create prediction model, echo=TRUE}

# set up new lists for output
model_2c_abundance <- list()
abundance_model <- list()
main_plot_abundance <- list()  

# run models for both species richness and total abundance
data_fig_one <- predict_climate_list[[1]]

# Build the model, initially adding random effects (1|SS)
model_2c_abundance <- lmerTest::lmer(log(Total_abundance) ~ standard_anom * Predominant_land_use + (1|SS), data = data_fig_one) 

# Output the model's R squared value
print(StatisticalModels::R2GLMER(model_2c_abundance))

# Add additional random effects (1|SSB)
model_2c_abundance <- lmerTest::lmer(log(Total_abundance) ~ standard_anom * Predominant_land_use + (1|SS) + (1|SSB), data = data_fig_one) 

# Output the R squared value again
print(StatisticalModels::R2GLMER(model_2c_abundance))
print(summary(model_2c_abundance))
print(paste("AIC:", AIC(model_2c_abundance)))

# 计算固定效应的置信区间
conf_intervals <- confint(model_2c_abundance, method = "profile", level = 0.95)

# 打印结果
print(conf_intervals)

# Run predictions for the standard anomaly model
abundance_model <- predict_continuous(
    model = model_2c_abundance,
    model_data = data_fig_one,
    response_variable = "Total_abundance",
    categorical_variable = "Predominant_land_use",
    continuous_variable = "standard_anom",
    continuous_transformation = "",
    random_variable = c("SS", "SSB", "SSBS")
)



```


```{r plot prediction model, echo=TRUE}

# Plot the effect of standardised temperature anomaly and land use type on abundance
main_plot_abundance <- ggplot(abundance_model) +
  geom_line(aes(x = standard_anom, y = y_value, colour = Predominant_land_use), size = 1.5) +
  geom_ribbon(aes(x = standard_anom, y = y_value, fill = Predominant_land_use, ymin = y_value_minus, ymax = y_value_plus), alpha = 0.4) +
  scale_fill_manual("Land-use type", values = c("#009E73", "#E69F00")) +
  scale_colour_manual("Land-use type", values = c("#009E73", "#E69F00")) +
  xlab("Standardised temperature anomaly") +
  ylab("Total abundance") +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  scale_y_continuous(limits = c(1, 6.5), breaks = c(1.609438, 2.302585, 2.995732, 3.6888795, 4.382027, 5.075174, 5.768321, 6.461468), labels = c(5, 10, 20, 40, 80, 160, 320, 640)) +
  scale_x_continuous(limits = c(-0.6, 2.88))

# Display the plot
print(main_plot_abundance)


```

```{r pcreate prediction model with spatial structure, echo=TRUE, results='hide'}

# Directly access the single dataset in the list
data_spatial <- predict_climate_list[[1]]
data_spatial <- na.omit(data_spatial)  # Remove rows with missing values

# Check for any remaining NA values
if(sum(is.na(data_spatial)) > 0) {
    print("Data set still contains NA values.")
} else {
    # Exponential spatial structure model
    model_exponential_random <- lme(
      log(Total_abundance) ~ standard_anom * Predominant_land_use,
      random = list(SS = ~ 1, SSB = ~ 1),
      data = data_spatial,
      correlation = corExp(form = ~ UTM_Easting + UTM_Northing)
    )
}
# 计算固定效应和随机效应的置信区间
model_intervals <- intervals(model_exponential_random, level = 0.95)

# 打印所有相关的置信区间
print(model_intervals)


abundance_model_exponential <- predict_continuous(
  model = model_exponential_random,
  model_data = data_spatial,
  response_variable = "Total_abundance",
  categorical_variable = c("Predominant_land_use"),
  continuous_variable = c("standard_anom"),
  continuous_transformation = "",
  random_variable = c("SS", "SSB")
)



```

```{r plot prediction model with spatial structure, echo=TRUE}

main_plot_abundance_exponential <- ggplot(abundance_model_exponential) +
  geom_line(aes(x = standard_anom, y = y_value, colour = Predominant_land_use), linewidth = 1.5) +
  geom_ribbon(aes(x = standard_anom, y = y_value, fill = Predominant_land_use, ymin = y_value_minus, ymax = y_value_plus), alpha = 0.4) +
  scale_fill_manual("Land-use type", values = c("#009E73", "#E69F00")) +
  scale_colour_manual("Land-use type", values = c("#009E73", "#E69F00")) +
  xlab("Standardised temperature anomaly") +
  ylab("Total abundance") +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  scale_y_continuous(breaks = c(1.609438, 2.302585, 2.995732, 3.6888795, 4.382027, 5.075174, 5.768321, 6.461468), 
                     labels = c(5, 10, 20, 40, 80, 160, 320, 640)) +
  coord_cartesian(xlim = c(-0.6, 2.88), ylim = c(1, 6.5))


print(main_plot_abundance_exponential)


```